{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218ba7f2",
   "metadata": {},
   "source": [
    "# Importing Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fcd0faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "11.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\ignite\\handlers\\checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "C:\\Windows\\Temp\\ipykernel_13788\\3892951699.py:44: DeprecationWarning: c:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\ignite\\contrib\\handlers\\tensorboard_logger.py has been moved to /ignite/handlers/tensorboard_logger.py and will be removed in version 0.6.0.\n",
      " Please refer to the documentation for more details.\n",
      "  from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1. Imports and Seed Setup\n",
    "# -----------------------------\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import EdgeConv, knn_graph, global_max_pool\n",
    "\n",
    "\n",
    "# PyTorch Ignite\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Loss\n",
    "from ignite.metrics.metric import Metric\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger\n",
    "\n",
    "# Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "\n",
    "\n",
    "print(torch.__version__)     # e.g., 2.1.0\n",
    "print(torch.version.cuda) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f71c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyG extensions loaded and working.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# PyG Extension Sanity Check\n",
    "# -----------------------------\n",
    "try:\n",
    "    import torch_scatter\n",
    "    import torch_sparse\n",
    "    import torch_cluster\n",
    "    import torch_spline_conv\n",
    "    from torch_geometric.nn import knn_graph\n",
    "\n",
    "    # Create dummy inputs\n",
    "    x = torch.randn(32, 3).cuda()  # 32 3D points\n",
    "    batch = torch.zeros(32, dtype=torch.long).cuda()\n",
    "    edge_index = knn_graph(x, k=4, batch=batch)\n",
    "\n",
    "    print(\"✅ PyG extensions loaded and working.\")\n",
    "except ImportError as e:\n",
    "    print(\"❌ PyG Import Error:\", e)\n",
    "except Exception as e:\n",
    "    print(\"❌ PyG Runtime Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e54c9f",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951a8a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Average Cd",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6310a3a9-1c40-4436-9c37-b425ba388a00",
       "rows": [
        [
         "count",
         "7713.0"
        ],
        [
         "mean",
         "0.2844122596453961"
        ],
        [
         "std",
         "0.03723220287049474"
        ],
        [
         "min",
         "0.201138367156146"
        ],
        [
         "25%",
         "0.255859366461794"
        ],
        [
         "50%",
         "0.282986553124688"
        ],
        [
         "75%",
         "0.311521421763092"
        ],
        [
         "max",
         "0.383329962159601"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7713.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.284412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.037232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.201138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.255859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.282987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.311521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.383330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Average Cd\n",
       "count  7713.000000\n",
       "mean      0.284412\n",
       "std       0.037232\n",
       "min       0.201138\n",
       "25%       0.255859\n",
       "50%       0.282987\n",
       "75%       0.311521\n",
       "max       0.383330"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('..\\data\\DrivAerNetPlusPlus_Drag_8k_cleaned.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c14b960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Using device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Memory Allocated: 0.0029296875 MB\n",
      "Memory Cached: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get current device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Print GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Memory Allocated:\", torch.cuda.memory_allocated(0) / 1024**2, \"MB\")\n",
    "    print(\"Memory Cached:\", torch.cuda.memory_reserved(0) / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fe6b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Average Cd', ylabel='Count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the pdf graph for Average Drag Coefficient (Cd)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Average Cd'], bins=30, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41516c9c",
   "metadata": {},
   "source": [
    "# Pepraring Scaler Function and Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a83241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler mean: 0.284506, std: 0.037448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/cd_scaler.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1.2. Prepaing Scaler\n",
    "# -----------------------------\n",
    "\n",
    "# Subset to only training car IDs\n",
    "with open(\"../data/subset_dir/train_design_ids.txt\") as f:\n",
    "    train_ids = [line.strip() for line in f]\n",
    "\n",
    "df_train = df[df[\"Design\"].isin(train_ids)]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[[\"Average Cd\"]])\n",
    "\n",
    "print(f\"Scaler mean: {scaler.mean_[0]:.6f}, std: {scaler.scale_[0]:.6f}\")\n",
    "\n",
    "# Save scaler to disk\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "joblib.dump(scaler, \"../outputs/cd_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0ed085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1.3. Global Variables\n",
    "# -----------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "model_checkpoint_dir=\"../outputs/checkpointsTransformer\"\n",
    "analysis_output_dir='../outputs/analysisTransformer'\n",
    "\n",
    "scaler = joblib.load(\"../outputs/cd_scaler.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da915d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20024180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################\n",
    "# Attention Pooling PointNet2D\n",
    "# ###################################\n",
    "class PointNet2D(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim=2, \n",
    "        emb_dim=256\n",
    "    ):\n",
    "        super(PointNet2D, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 64, 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(128, emb_dim, 1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        # Attention layer: produces (B, 1, N)\n",
    "        self.attn = nn.Conv1d(emb_dim, 1, 1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (B, N, 2)\n",
    "        x = x.transpose(1, 2)  # (B, 2, N)\n",
    "\n",
    "        features = self.mlp(x)  # (B, emb_dim, N)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_logits = self.attn(features)  # (B, 1, N)\n",
    "        \n",
    "        \n",
    "        # Masking\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # (B, 1, N)\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_logits, dim=2)  # (B, 1, N)\n",
    "\n",
    "        # Apply attention weights\n",
    "        embedding = torch.sum(features * attn_weights, dim=2)  # (B, emb_dim)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f350841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################\n",
    "# Minimal EdgeConv Slice Encoder (Replaces PointNet2D)\n",
    "# ###################################\n",
    "class EdgeConvSliceEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a batch of 2D slices, each with a variable number of points,\n",
    "    into a fixed-size embedding vector for each slice.\n",
    "    This module is designed to work with PyTorch Geometric's Batch object.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, emb_dim=256, k=20):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "        # Define the MLP that will be used inside EdgeConv\n",
    "        # It processes concatenated features of a point and its neighbors\n",
    "        mlp = nn.Sequential(\n",
    "            nn.Linear(2 * input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, emb_dim) # Output the final embedding dimension\n",
    "        )\n",
    "\n",
    "        # The EdgeConv layer itself\n",
    "        self.edge_conv = EdgeConv(nn=mlp, aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Processes a PyG Batch object representing multiple slices.\n",
    "        Args:\n",
    "            data (torch_geometric.data.Batch): A batch of slices, containing:\n",
    "                - data.x (Tensor): All points from all slices concatenated [N_total_points, 2].\n",
    "                - data.batch (Tensor): A tensor mapping each point to its original slice\n",
    "                                    in the batch [N_total_points].\n",
    "        Returns:\n",
    "            Tensor: A tensor of shape [num_slices_in_batch, emb_dim].\n",
    "        \"\"\"\n",
    "        # 1. Dynamically build the k-NN graph for the current set of points.\n",
    "        #    `knn_graph` is batch-aware and will not connect points from different slices.\n",
    "        edge_index = knn_graph(data.x, k=self.k, batch=data.batch)\n",
    "\n",
    "        # 2. Apply the EdgeConv layer to learn features for each point.\n",
    "        point_features = self.edge_conv(data.x, edge_index)\n",
    "\n",
    "        # 3. Use global max pooling to get ONE fixed-size vector per slice.\n",
    "        #    `global_max_pool` is also batch-aware.\n",
    "        slice_embedding = global_max_pool(point_features, data.batch)\n",
    "\n",
    "        return slice_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0225096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################\n",
    "# Attention Pooling Transformer Encoder\n",
    "# ################################\n",
    "# --- Your helper function (define this once outside the class) ---\n",
    "def generate_sinusoidal_position_embeddings(max_seq_len, embedding_dim):\n",
    "    pe = torch.zeros(max_seq_len, embedding_dim)\n",
    "    position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe.unsqueeze(0) # Add batch dimension for (1, S, D)\n",
    "\n",
    "# --- Your TransformerSliceEncoder class ---\n",
    "class TransformerSliceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 256, \n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        nhead: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        max_seq_len: int = 80, # Fixed at 80\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # KEY CHANGE 1: Use the pre-calculated sinusoidal embeddings\n",
    "        # We register it as a buffer so it's moved to GPU with the model,\n",
    "        # but NOT optimized by the optimizer (fixed).\n",
    "        self.register_buffer(\n",
    "            'pos_encoder',\n",
    "            generate_sinusoidal_position_embeddings(max_seq_len, input_dim)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_dim * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.attn_pool = nn.Linear(input_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_key_padding_mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        x: (B, S, D)\n",
    "        src_key_padding_mask: (B, S) mask with True = pad, False = valid\n",
    "        \"\"\"\n",
    "        B, S, D = x.shape\n",
    "        # KEY CHANGE 2: No change here, the addition logic remains the same.\n",
    "        # Now, self.pos_encoder contains fixed, meaningful position info.\n",
    "        x = x + self.pos_encoder[:, :S, :]\n",
    "\n",
    "        out = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Attention pooling\n",
    "        scores = self.attn_pool(out).squeeze(-1)  # (B, S)\n",
    "        \n",
    "        if src_key_padding_mask is not None:\n",
    "            scores = scores.masked_fill(src_key_padding_mask, -1e9)\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # (B, S)\n",
    "        pooled = torch.sum(attn_weights.unsqueeze(-1) * out, dim=1)  # (B, D)\n",
    "\n",
    "        return self.dropout(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "824a7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################\n",
    "# Regression Model for Cd\n",
    "# ################################\n",
    "\n",
    "class CdRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=256):  \n",
    "        \"\"\"\n",
    "        A simple regression model to predict the average drag coefficient (Cd) from the output of the TransformerSliceEncoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 1)  # Output layer for regression\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, input_dim)\n",
    "        returns: (B,)\n",
    "        \"\"\"\n",
    "        return self.net(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3870d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL PREDICTOR NET\n",
    "\n",
    "\n",
    "# class CdPredictorNet(nn.Module):\n",
    "#     def __init__(self, pointnet, transformer_encoder, regressor):\n",
    "#         super().__init__()\n",
    "#         self.pointnet = pointnet\n",
    "#         self.transformer_encoder = transformer_encoder\n",
    "#         self.regressor = regressor\n",
    "\n",
    "#     def forward(self, slices, point_mask, slice_mask=None):\n",
    "#         \"\"\"\n",
    "#         slices: (B, S, N, D) — batch of S slices, N points each, D = 2 (xy)\n",
    "#         point_mask: (B, S, N) — mask for points (1 for valid, 0 for padded)\n",
    "#         slice_mask: (B, S) — mask for slices (optional, 1 valid / 0 padded)\n",
    "#         \"\"\"\n",
    "#         B, S, N, D = slices.shape\n",
    "\n",
    "#         # Flatten slices for PointNet\n",
    "#         flat_slices = slices.view(B * S, N, D)         # (B*S, N, D)\n",
    "#         flat_mask = point_mask.view(B * S, N)          # (B*S, N)\n",
    "\n",
    "#         # PointNet: (B*S, N, D) -> (B*S, emb_dim)\n",
    "#         slice_embs = self.pointnet(flat_slices, flat_mask)  \n",
    "#         slice_embs = slice_embs.view(B, S, -1)         # (B, S, emb_dim)\n",
    "\n",
    "#         # Transformer Encoder: (B, S, emb_dim) -> (B, final_emb_dim)\n",
    "#         car_emb = self.transformer_encoder(slice_embs)\n",
    "\n",
    "#         # Regression head\n",
    "#         return self.regressor(car_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "020da62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTOR NET FOR EXPERIMENTING EDGE CONV\n",
    "class CdPredictorNet(nn.Module):\n",
    "    def __init__(self, slice_encoder, transformer_encoder, regressor):\n",
    "        super().__init__()\n",
    "        self.slice_encoder = slice_encoder\n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def forward(self, car_slice_batches):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            car_slice_batches (list of torch_geometric.data.Batch):\n",
    "                A list of length 80. Each element is a PyG Batch object\n",
    "                representing all slices at that index from the main training batch.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        slice_embeddings = []\n",
    "\n",
    "        for slice_batch in car_slice_batches:\n",
    "            slice_batch = slice_batch.to(device)\n",
    "            embedding = self.slice_encoder(slice_batch)\n",
    "            slice_embeddings.append(embedding)\n",
    "\n",
    "        transformer_input = torch.stack(slice_embeddings, dim=0)\n",
    "        transformer_input = transformer_input.transpose(0, 1)\n",
    "\n",
    "        car_emb = self.transformer_encoder(transformer_input)\n",
    "\n",
    "        return self.regressor(car_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fcc65",
   "metadata": {},
   "source": [
    "# Dataset loader and Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385df74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Dataset Loader\n",
    "# -----------------------------\n",
    "class CarSlicesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids_txt, npz_dir, csv_path, max_cars=None, scaler = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ids_txt (str): Path to the text file containing car IDs.\n",
    "            npz_dir (str): Directory containing the .npz files.\n",
    "            csv_path (str): Path to the CSV file with Cd values.\n",
    "            max_cars (int, optional): Limit the number of cars to load. Defaults to None.\n",
    "            scaler (object, optional): Scaler object for normalizing Cd values. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.car_ids = [line.strip() for line in open(ids_txt)]\n",
    "        if max_cars:\n",
    "            self.car_ids = self.car_ids[:max_cars]\n",
    "        self.npz_dir = npz_dir\n",
    "        self.cd_map = pd.read_csv(csv_path).set_index(\"Design\")[\"Average Cd\"].to_dict()\n",
    "        self.scaler = scaler if scaler else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.car_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        car_id = self.car_ids[idx]\n",
    "        data = np.load(os.path.join(self.npz_dir, f\"{car_id}_axis-x.npz\"))\n",
    "\n",
    "        # Keep in NumPy for now (better for pin_memory and batch collation)\n",
    "        slices = data[\"slices\"].astype(np.float32)         # (80, 6500, 2)\n",
    "        point_mask = data[\"point_mask\"].astype(np.float32) # (80, 6500)\n",
    "        slice_mask = data[\"slice_mask\"].astype(np.float32) # (80,)\n",
    "        cd_value = np.float32(self.cd_map[car_id])\n",
    "\n",
    "        if self.scaler:\n",
    "            cd_value = self.scaler.transform([[cd_value]])[0, 0]\n",
    "\n",
    "        return slices, point_mask, slice_mask, cd_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f192830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADER DIRECT SLICES\n",
    "# ################################\n",
    "# New Dataset for Raw Slices (Replaces CarSlicesDataset)\n",
    "# ################################\n",
    "class CarSlicesDataset_PyG(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids_txt, raw_npy_dir, csv_path, scaler=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ids_txt (str): Path to the text file containing car IDs.\n",
    "            raw_npy_dir (str): Directory containing the RAW .npy files (object arrays).\n",
    "            csv_path (str): Path to the CSV file with Cd values.\n",
    "            scaler (object, optional): Scaler for normalizing Cd values.\n",
    "        \"\"\"\n",
    "        self.car_ids = [line.strip() for line in open(ids_txt)]\n",
    "        self.raw_npy_dir = raw_npy_dir\n",
    "        self.cd_map = pd.read_csv(csv_path).set_index(\"Design\")[\"Average Cd\"].to_dict()\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.car_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        car_id = self.car_ids[idx]\n",
    "        raw_slices = np.load(os.path.join(self.raw_npy_dir, f\"{car_id}_axis-x.npy\"), allow_pickle=True)\n",
    "        cd_value = np.float32(self.cd_map[car_id])\n",
    "\n",
    "        slice_data_list = [\n",
    "            Data(x=torch.from_numpy(s.astype(np.float32))) for s in raw_slices\n",
    "        ]\n",
    "\n",
    "        if self.scaler:\n",
    "            cd_value = self.scaler.transform([[cd_value]])[0, 0]\n",
    "\n",
    "        return slice_data_list, cd_value\n",
    "\n",
    "# ################################\n",
    "# New Collate Function for the DataLoader\n",
    "# ################################\n",
    "# Simplified Collate Function\n",
    "def collate_fn_pyg(batch):\n",
    "    car_data_list, cd_list = zip(*batch)\n",
    "    slices_by_index = zip(*car_data_list)\n",
    "    batched_slices = [Batch.from_data_list(slice_list) for slice_list in slices_by_index]\n",
    "    cd_values = torch.tensor(cd_list, dtype=torch.float32)\n",
    "    return batched_slices, cd_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ffc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################################\n",
    "# SliceEdgeNet: A Deeper, Dynamic Slice Encoder\n",
    "# ###################################\n",
    "class SliceEdgeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer dynamic graph CNN to learn rich geometric features from 2D slices.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, emb_dim=256, k=16):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "\n",
    "        # First convolutional layer: Learns basic geometric primitives.\n",
    "        # Takes raw coordinates (dim=2*2=4) and outputs 64-dim features.\n",
    "        mlp1 = nn.Sequential(\n",
    "            nn.Linear(2 * input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv1 = EdgeConv(nn=mlp1, aggr='max')\n",
    "\n",
    "        # Second convolutional layer: Learns higher-order motifs from the first layer's features.\n",
    "        # Takes concatenated 64-dim features (dim=2*64=128) and outputs the final embedding dim.\n",
    "        mlp2 = nn.Sequential(\n",
    "            nn.Linear(2 * 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, emb_dim)\n",
    "        )\n",
    "        self.conv2 = EdgeConv(nn=mlp2, aggr='max')\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Processes a PyG Batch object.\n",
    "        \"\"\"\n",
    "        # --- First Layer ---\n",
    "        # Graph is built on spatial coordinates (data.x)\n",
    "        edge_index1 = knn_graph(data.x, k=self.k, batch=data.batch)\n",
    "        # Output x1 has shape [N_total_points, 64]\n",
    "        x1 = self.conv1(data.x, edge_index1)\n",
    "\n",
    "        # --- Second Layer (The \"Dynamic\" Part) ---\n",
    "        # The new graph is built on the FEATURE SPACE (x1) from the first layer.\n",
    "        # This allows the model to group points that have similar geometric roles.\n",
    "        edge_index2 = knn_graph(x1, k=self.k, batch=data.batch)\n",
    "        # Output x2 has shape [N_total_points, 256]\n",
    "        x2 = self.conv2(x1, edge_index2)\n",
    "\n",
    "        # --- Final Global Pooling ---\n",
    "        # Aggregate all the rich point features into one vector per slice.\n",
    "        slice_embedding = global_max_pool(x2, data.batch)\n",
    "        \n",
    "        return slice_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80ec6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# pointnet = PointNet2D()\n",
    "# encoder = TransformerSliceEncoder()\n",
    "# regressor = CdRegressor()\n",
    "# model = CdPredictorNet(pointnet, encoder, regressor).to(device)\n",
    "\n",
    "# summary(\n",
    "#     model,\n",
    "#     input_data=[\n",
    "#         torch.zeros(batch_size, 80, 6500, 2).to(device),   # pointnet input\n",
    "#         torch.ones(batch_size, 80, 6500).to(device),       # mask?\n",
    "#         torch.ones(batch_size, 80).to(device)              # maybe slice mask?\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75f8f7",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eddcebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RMSE metric\n",
    "class RMSE(Metric):\n",
    "    def reset(self):\n",
    "        self._sum_squared_error = 0.0\n",
    "        self._num_examples = 0\n",
    "\n",
    "    def update(self, output):\n",
    "        y_pred, y = output\n",
    "        errors = (y_pred - y) ** 2\n",
    "        self._sum_squared_error += torch.sum(errors).item()\n",
    "        self._num_examples += y.shape[0]\n",
    "\n",
    "    def compute(self):\n",
    "        return (self._sum_squared_error / self._num_examples) ** 0.5\n",
    "\n",
    "# Custom R² metric\n",
    "class R2Score(Metric):\n",
    "    def reset(self):\n",
    "        self._y_true = []\n",
    "        self._y_pred = []\n",
    "\n",
    "    def update(self, output):\n",
    "        y_pred, y = output\n",
    "        self._y_pred.extend(y_pred.detach().cpu().numpy().flatten())\n",
    "        self._y_true.extend(y.detach().cpu().numpy().flatten())\n",
    "\n",
    "    def compute(self):\n",
    "        return r2_score(self._y_true, self._y_pred)\n",
    "\n",
    "# Create training engine\n",
    "# def create_trainer(model, optimizer, loss_fn, device, accumulation_steps):\n",
    "#     def _update(engine, batch):\n",
    "#         model.train()\n",
    "#         slices, point_mask, slice_mask, cd_gt = [b.to(device) for b in batch]\n",
    "#         optimizer.zero_grad()\n",
    "#         with torch.amp.autocast(\"cuda\"):\n",
    "#             pred = model(slices, point_mask, slice_mask)\n",
    "#             # scale loss for accumulation\n",
    "#             loss = loss_fn(pred, cd_gt.float()) / accumulation_steps\n",
    "#         loss.backward()\n",
    "#         if (engine.state.iteration % accumulation_steps) == 0:\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "#         return pred, cd_gt\n",
    "#     return Engine(_update)\n",
    "\n",
    "# # Create evaluation engine\n",
    "# def create_evaluator(model, loss_fn, device):\n",
    "#     def _inference(engine, batch):\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             slices, point_mask, slice_mask, cd_gt = [b.to(device) for b in batch]\n",
    "#             with torch.amp.autocast(\"cuda\"):\n",
    "#                 pred = model(slices, point_mask, slice_mask)\n",
    "#         return pred, cd_gt\n",
    "\n",
    "#     return Engine(_inference)\n",
    "\n",
    "# --- MODIFIED: Create training engine ---\n",
    "def create_trainer(model, optimizer, loss_fn, device, accumulation_steps):\n",
    "    def _update(engine, batch):\n",
    "        model.train()\n",
    "        # NEW: Unpack the simplified batch format\n",
    "        car_slice_batches, cd_gt = batch\n",
    "        cd_gt = cd_gt.to(device) # Move only the GT tensor to the device here\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            # NEW: Pass only the list of batches to the model\n",
    "            pred = model(car_slice_batches)\n",
    "            loss = loss_fn(pred, cd_gt.float()) / accumulation_steps\n",
    "        loss.backward()\n",
    "        if (engine.state.iteration % accumulation_steps) == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        return pred, cd_gt\n",
    "    return Engine(_update)\n",
    "\n",
    "# --- MODIFIED: Create evaluation engine ---\n",
    "def create_evaluator(model, loss_fn, device):\n",
    "    def _inference(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # NEW: Unpack the simplified batch format\n",
    "            car_slice_batches, cd_gt = batch\n",
    "            cd_gt = cd_gt.to(device) # Move only the GT tensor to the device here\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                # NEW: Pass only the list of batches to the model\n",
    "                pred = model(car_slice_batches)\n",
    "        return pred, cd_gt\n",
    "\n",
    "    return Engine(_inference)\n",
    "\n",
    "# Attach metrics to an engine\n",
    "def attach_metrics(engine, loss_fn):\n",
    "    Loss(loss_fn).attach(engine, \"loss\")\n",
    "    RMSE().attach(engine, \"rmse\")\n",
    "    R2Score().attach(engine, \"r2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6449f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def train_model(\n",
    "    resume: bool = True,\n",
    "    delete_previous_model_dir: bool = False,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 1,\n",
    "    learning_rate: float = 1e-4,\n",
    "    accumulation_steps: int = 32,\n",
    "    early_stopping_patience: int = 5,\n",
    "    model_dir: str = \"./model_dir\",\n",
    "\n",
    "    # Dataset configuration\n",
    "    training_set_size: int = None,\n",
    "    validation_set_size: int = None,\n",
    "    train_ids_txt: str = \"../data/subset_dir/train_design_ids.txt\",\n",
    "    val_ids_txt: str = \"../data/subset_dir/val_design_ids.txt\",\n",
    "    # npz_dir: str = \"../outputs/pad_masked_slices\",\n",
    "    # --- MODIFIED: Point to the RAW .npy directory ---\n",
    "    raw_npy_dir: str = \"../outputs/slices\",\n",
    "    csv_path: str = \"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\",\n",
    "    scaler=None,\n",
    "\n",
    "    # # Model hyperparameters\n",
    "    # point_net_input_dim: int = 2,\n",
    "    # point_net_embedding_dim: int = 256,\n",
    "    # Model hyperparameters\n",
    "    # --- NEW: Hyperparameter for EdgeConv ---\n",
    "    k_neighbors: int = 20,\n",
    "    slice_embedding_dim: int = 256,\n",
    "    transformer_hidden_dim: int = 256,\n",
    "    transformer_num_layers: int = 2,\n",
    "    transformer_nhead: int = 1,\n",
    "    cd_regressor_input_dim: int = 256,\n",
    "):\n",
    "    checkpoints_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "    results_dir = os.path.join(model_dir, \"results\")\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Optional cleanup\n",
    "    if delete_previous_model_dir and not resume:\n",
    "        if os.path.isdir(model_dir) and os.listdir(model_dir):\n",
    "            confirm = input(f\"[WARN] Contents found in '{model_dir}'. Delete all contents? (yes/no): \").strip().lower()\n",
    "            if confirm == \"yes\":\n",
    "                # Delete all contents inside model_dir\n",
    "                for entry in os.scandir(model_dir):\n",
    "                    if entry.is_dir():\n",
    "                        shutil.rmtree(entry.path)\n",
    "                    else:\n",
    "                        os.remove(entry.path)\n",
    "                print(f\"[INFO] All contents of '{model_dir}' have been deleted.\")\n",
    "            else:\n",
    "                print(\"[INFO] Cleanup aborted by user.\")\n",
    "        else:\n",
    "            print(f\"[INFO] Nothing to clean. '{model_dir}' is empty or does not exist.\")\n",
    "\n",
    "\n",
    "    # # 2) Model setup\n",
    "    # pointnet = PointNet2D(input_dim=point_net_input_dim, emb_dim=point_net_embedding_dim)\n",
    "    # encoder = TransformerSliceEncoder(input_dim=point_net_embedding_dim, hidden_dim=transformer_hidden_dim, num_layers=transformer_num_layers, nhead=transformer_nhead)\n",
    "    # regressor = CdRegressor(input_dim=cd_regressor_input_dim)\n",
    "    # model = CdPredictorNet(pointnet, encoder, regressor).to(device)\n",
    "    \n",
    "    # --- MODIFIED: Model setup ---\n",
    "    print(f\"[INFO] Setting up model with k={k_neighbors} for EdgeConv...\")\n",
    "    slice_encoder = EdgeConvSliceEncoder(input_dim=2, emb_dim=slice_embedding_dim, k=k_neighbors)\n",
    "    # print(f\"[INFO] Setting up model with SliceEdgeNet (k={k_neighbors})...\")\n",
    "    # slice_encoder = SliceEdgeNet(input_dim=2, emb_dim=slice_embedding_dim, k=k_neighbors)\n",
    "    encoder = TransformerSliceEncoder(input_dim=slice_embedding_dim, hidden_dim=transformer_hidden_dim, num_layers=transformer_num_layers, nhead=transformer_nhead)\n",
    "    regressor = CdRegressor(input_dim=cd_regressor_input_dim)\n",
    "    model = CdPredictorNet(slice_encoder, encoder, regressor).to(device)\n",
    "\n",
    "\n",
    "    print(model)\n",
    "    print(f\"[INFO] Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # 3) Training components\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # --- ADD THIS: The Scheduler ---\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)\n",
    "    # We will monitor Validation Loss, so mode='min'\n",
    "    # factor=0.2: Reduce LR by a factor of 5 when triggered.\n",
    "    # patience=3: Wait 3 epochs with no improvement in Val_loss before reducing LR.\n",
    "    \n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    trainer = create_trainer(model, optimizer, loss_fn, device, accumulation_steps)\n",
    "    evaluator = create_evaluator(model, loss_fn, device)\n",
    "    attach_metrics(trainer, loss_fn)\n",
    "    attach_metrics(evaluator, loss_fn)\n",
    "\n",
    "    train_pbar = ProgressBar(persist=True, desc=\"Training\")\n",
    "    train_pbar.attach(trainer, metric_names=[\"loss\", \"rmse\", \"r2\"])\n",
    "    val_pbar = ProgressBar(persist=True, desc=\"Validation\")\n",
    "    val_pbar.attach(evaluator, metric_names=[\"loss\", \"rmse\", \"r2\"])\n",
    "\n",
    "    # # 4) Data\n",
    "    # train_loader = torch.utils.data.DataLoader(\n",
    "    #     CarSlicesDataset(train_ids_txt, npz_dir, csv_path, training_set_size, scaler),\n",
    "    #     batch_size=batch_size, shuffle=True)\n",
    "    # val_loader = torch.utils.data.DataLoader(\n",
    "    #     CarSlicesDataset(val_ids_txt, npz_dir, csv_path, validation_set_size, scaler),\n",
    "    #     batch_size=batch_size * 2, shuffle=False)\n",
    "\n",
    "    # --- MODIFIED: Data setup ---\n",
    "    print(f\"[INFO] Loading data from raw npy directory: {raw_npy_dir}\")\n",
    "    train_dataset = CarSlicesDataset_PyG(train_ids_txt, raw_npy_dir, csv_path, scaler)\n",
    "    val_dataset = CarSlicesDataset_PyG(val_ids_txt, raw_npy_dir, csv_path, scaler)\n",
    "\n",
    "    train_loader = PyGDataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn_pyg,\n",
    "    )\n",
    "    val_loader = PyGDataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size * 2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn_pyg,\n",
    "    )\n",
    "\n",
    "    # 5) TensorBoard\n",
    "    tb_logger = TensorboardLogger(log_dir=os.path.join(checkpoints_dir, \"logs\"))\n",
    "    for tag, engine in [(\"training\", trainer), (\"validation\", evaluator)]:\n",
    "        tb_logger.attach_output_handler(engine, Events.EPOCH_COMPLETED, tag, metric_names=[\"loss\", \"rmse\", \"r2\"])\n",
    "\n",
    "    # 6) Resume\n",
    "    start_epoch = 1\n",
    "    if resume:\n",
    "        ckpts = sorted(glob.glob(os.path.join(checkpoints_dir, \"epoch_*.pt\")))\n",
    "        if ckpts:\n",
    "            last_ckpt = ckpts[-1]\n",
    "            print(f\"[INFO] Resuming from checkpoint: {os.path.basename(last_ckpt)}\")\n",
    "            data = torch.load(last_ckpt, map_location=device)\n",
    "            model.load_state_dict(data[\"model\"])\n",
    "            optimizer.load_state_dict(data[\"optimizer\"])\n",
    "            start_epoch = data.get(\"epoch\", 1) + 1\n",
    "        else:\n",
    "            print(\"[INFO] No checkpoint found. Starting fresh.\")\n",
    "\n",
    "    # 7) Best model saving\n",
    "    evaluator.add_event_handler(Events.COMPLETED, ModelCheckpoint(\n",
    "        dirname=checkpoints_dir,\n",
    "        filename_prefix=\"best_model\",\n",
    "        n_saved=1,\n",
    "        score_function=lambda eng: eng.state.metrics[\"r2\"],\n",
    "        score_name=\"r2\",\n",
    "        global_step_transform=lambda eng, _: eng.state.epoch + start_epoch - 1,\n",
    "        require_empty=False\n",
    "    ), {\"model\": model})\n",
    "\n",
    "    # 8) Early stopping\n",
    "    evaluator.add_event_handler(Events.COMPLETED, EarlyStopping(\n",
    "        patience=early_stopping_patience,\n",
    "        score_function=lambda eng: eng.state.metrics[\"r2\"],\n",
    "        trainer=trainer\n",
    "    ))\n",
    "\n",
    "    # 9) Load previous history if exists\n",
    "    history_path = os.path.join(results_dir, \"training_history.json\")\n",
    "    new_history = []\n",
    "    if os.path.exists(history_path):\n",
    "        with open(history_path, \"r\") as f:\n",
    "            new_history = json.load(f)\n",
    "    else:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(history_path), exist_ok=True)\n",
    "\n",
    "    # 10) Validation handler\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def validate(engine):\n",
    "        evaluator.run(val_loader)\n",
    "        epoch_num = engine.state.epoch + start_epoch - 1\n",
    "        train_metrics = engine.state.metrics\n",
    "        # --- ADD THIS: Tell the scheduler to step ---\n",
    "        scheduler.step(val_metrics['loss'])\n",
    "        val_metrics = evaluator.state.metrics\n",
    "\n",
    "        record = {\n",
    "            \"epoch\": epoch_num,\n",
    "            \"Train_loss\": train_metrics[\"loss\"],\n",
    "            \"Train_rmse\": train_metrics[\"rmse\"],\n",
    "            \"Train_r2\": train_metrics[\"r2\"],\n",
    "            \"Val_loss\": val_metrics[\"loss\"],\n",
    "            \"Val_rmse\": val_metrics[\"rmse\"],\n",
    "            \"Val_r2\": val_metrics[\"r2\"],\n",
    "        }\n",
    "        new_history.append(record)\n",
    "\n",
    "        print(f\"\\n[Epoch {epoch_num:03d}] ------------------------\")\n",
    "        for key, value in record.items():\n",
    "            if key != \"epoch\":\n",
    "                print(f\"{key:17}: {value:.6f}\" if isinstance(value, float) else f\"{key:17}: {value}\")\n",
    "\n",
    "        with open(history_path + \".bak\", \"w\") as f:\n",
    "            json.dump(new_history, f, indent=2)\n",
    "        with open(history_path, \"w\") as f:\n",
    "            json.dump(new_history, f, indent=2)\n",
    "\n",
    "        torch.save({\n",
    "            \"epoch\": epoch_num,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()\n",
    "        }, os.path.join(checkpoints_dir, f\"epoch_{epoch_num:03d}_loss_{val_metrics['loss']:.8f}.pt\"))\n",
    "\n",
    "    # 11) Run training\n",
    "    try:\n",
    "        trainer.run(train_loader, max_epochs=num_epochs - start_epoch + 1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⛔ Training interrupted by user.\")\n",
    "    finally:\n",
    "        tb_logger.close()\n",
    "        best_ckpt = sorted(glob.glob(os.path.join(checkpoints_dir, \"best_model_*.pt\")))\n",
    "        if best_ckpt:\n",
    "            print(f\"[INFO] Loading best model from: {os.path.basename(best_ckpt[-1])}\")\n",
    "            best_model_data = torch.load(best_ckpt[-1], map_location=device)\n",
    "            model.load_state_dict(best_model_data[\"model\"] if \"model\" in best_model_data else best_model_data)\n",
    "\n",
    "        with open(history_path, \"r\") as f:\n",
    "            final_history = json.load(f)\n",
    "        return model, final_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f8fb3",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8541b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] All contents of '../model_outputs/DTM256_Model' have been deleted.\n",
      "[INFO] Setting up model with k=16 for EdgeConv...\n",
      "CdPredictorNet(\n",
      "  (slice_encoder): EdgeConvSliceEncoder(\n",
      "    (edge_conv): EdgeConv(nn=Sequential(\n",
      "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=256, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (transformer_encoder): TransformerSliceEncoder(\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (attn_pool): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (regressor): CdRegressor(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO] Total trainable parameters: 2,249,154\n",
      "[INFO] Loading data from raw npy directory: ../outputs/slices\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2141efcf4c84992ab5f371c090e622a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training[1/1350]   0%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f3a610e7204aeeada2c8e548145fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation[1/145]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: local variable 'val_metrics' referenced before assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading best model from: best_model_model_1_r2=-0.0131.pt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../model_outputs/DTM256_Model\\\\results\\\\training_history.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 208\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(resume, delete_previous_model_dir, num_epochs, batch_size, learning_rate, accumulation_steps, early_stopping_patience, model_dir, training_set_size, validation_set_size, train_ids_txt, val_ids_txt, raw_npy_dir, csv_path, scaler, k_neighbors, slice_embedding_dim, transformer_hidden_dim, transformer_num_layers, transformer_nhead, cd_regressor_input_dim)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\ignite\\engine\\engine.py:905\u001b[0m, in \u001b[0;36mEngine.run\u001b[1;34m(self, data, max_epochs, epoch_length)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_resume_enabled:\n\u001b[1;32m--> 905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\ignite\\engine\\engine.py:948\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n",
      "File \u001b[1;32mc:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\ignite\\engine\\engine.py:1023\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1023\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\ignite\\engine\\engine.py:660\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 660\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\ignite\\engine\\engine.py:979\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    978\u001b[0m handlers_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 979\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fire_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCH_COMPLETED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m epoch_time_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m handlers_start_time\n",
      "File \u001b[1;32mc:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\ignite\\engine\\engine.py:435\u001b[0m, in \u001b[0;36mEngine._fire_event\u001b[1;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m first, others \u001b[38;5;241m=\u001b[39m ((args[\u001b[38;5;241m0\u001b[39m],), args[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;28;01mif\u001b[39;00m (args \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m ((), args)\n\u001b[1;32m--> 435\u001b[0m func(\u001b[38;5;241m*\u001b[39mfirst, \u001b[38;5;241m*\u001b[39m(event_args \u001b[38;5;241m+\u001b[39m others), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[19], line 176\u001b[0m, in \u001b[0;36mtrain_model.<locals>.validate\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# --- ADD THIS: Tell the scheduler to step ---\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(\u001b[43mval_metrics\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    177\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmetrics\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'val_metrics' referenced before assignment",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m json_history_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_history.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Call the training function\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# General training configuration\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelete_previous_model_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Dataset configuration\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_set_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_set_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ids_txt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/subset_dir/train_design_ids.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_ids_txt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/subset_dir/val_design_ids.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# npz_dir=\"../outputs/pad_masked_slices\",\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# --- MODIFIED: Dataset configuration ---\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_npy_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../outputs/slices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# UPDATE THIS to your raw .npy file directory\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Model hyperparameters\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# point_net_input_dim=2,\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# point_net_embedding_dim=256,\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# --- MODIFIED: Model hyperparameters ---\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The new hyperparameter for EdgeConv\u001b[39;49;00m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_embedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer_hidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer_num_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformer_nhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcd_regressor_input_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Load Json history and convert to DataFrame and print\u001b[39;00m\n\u001b[0;32m     48\u001b[0m history_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(json_history_path)\n",
      "Cell \u001b[1;32mIn[19], line 219\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(resume, delete_previous_model_dir, num_epochs, batch_size, learning_rate, accumulation_steps, early_stopping_patience, model_dir, training_set_size, validation_set_size, train_ids_txt, val_ids_txt, raw_npy_dir, csv_path, scaler, k_neighbors, slice_embedding_dim, transformer_hidden_dim, transformer_num_layers, transformer_nhead, cd_regressor_input_dim)\u001b[0m\n\u001b[0;32m    216\u001b[0m     best_model_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(best_ckpt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    217\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m best_model_data \u001b[38;5;28;01melse\u001b[39;00m best_model_data)\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhistory_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    220\u001b[0m     final_history \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, final_history\n",
      "File \u001b[1;32mc:\\Users\\absaa\\anaconda3\\envs\\torch-gpu-copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../model_outputs/DTM256_Model\\\\results\\\\training_history.json'"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "model_dir = \"../model_outputs/DTM256_Model\"\n",
    "checkpoints_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "results_dir = os.path.join(model_dir, \"results\")\n",
    "os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "os.makedirs\n",
    "# History file paths\n",
    "json_history_path = os.path.join(results_dir, \"training_history.json\")\n",
    "\n",
    "# Call the training function\n",
    "model, _ = train_model(\n",
    "    # General training configuration\n",
    "    resume=False,\n",
    "    delete_previous_model_dir=True,\n",
    "    num_epochs=100,\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-4,\n",
    "    accumulation_steps=1,\n",
    "    early_stopping_patience=8,\n",
    "    model_dir = model_dir,\n",
    "\n",
    "    # Dataset configuration\n",
    "    training_set_size= None,\n",
    "    validation_set_size= None,\n",
    "    train_ids_txt=\"../data/subset_dir/train_design_ids.txt\",\n",
    "    val_ids_txt=\"../data/subset_dir/val_design_ids.txt\",\n",
    "    # npz_dir=\"../outputs/pad_masked_slices\",\n",
    "    # --- MODIFIED: Dataset configuration ---\n",
    "    raw_npy_dir=\"../outputs/slices\", # UPDATE THIS to your raw .npy file directory\n",
    "    csv_path=\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\",\n",
    "    scaler=scaler,\n",
    "\n",
    "    # Model hyperparameters\n",
    "    # point_net_input_dim=2,\n",
    "    # point_net_embedding_dim=256,\n",
    "    # --- MODIFIED: Model hyperparameters ---\n",
    "    k_neighbors=16, # The new hyperparameter for EdgeConv\n",
    "    slice_embedding_dim=256,\n",
    "    transformer_hidden_dim=256,\n",
    "    transformer_num_layers=4,\n",
    "    transformer_nhead=4,\n",
    "    cd_regressor_input_dim=256,\n",
    ")\n",
    "\n",
    "# Load Json history and convert to DataFrame and print\n",
    "history_df = pd.read_json(json_history_path)\n",
    "print(history_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92070095",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_all_checkpoints(checkpoint_dir=\"../outputs/temp_checkpoints\", output_dir=\"../outputs/temp_analysis\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_paths = sorted(glob.glob(os.path.join(checkpoint_dir, \"epoch_*.pt\")))\n",
    "    all_epoch_losses = []\n",
    "    epoch_numbers = []\n",
    "\n",
    "    print(\"\\n📋 Epoch-wise Loss Summary:\")\n",
    "    for ckpt_path in checkpoint_paths:\n",
    "        try:\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            losses = checkpoint.get(\"epoch_losses\", [])\n",
    "            epoch = checkpoint.get(\"epoch\", None)\n",
    "\n",
    "            if epoch is not None and losses:\n",
    "                epoch_numbers.append(epoch)\n",
    "                last_loss = losses[-1]\n",
    "                all_epoch_losses.append(last_loss)\n",
    "                print(f\"Epoch {epoch:02d} → Loss: {last_loss:.7f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipped {ckpt_path}: {e}\")\n",
    "\n",
    "    if not epoch_numbers:\n",
    "        print(\"❌ No valid checkpoints found.\")\n",
    "        return\n",
    "\n",
    "    # Save compiled loss log\n",
    "    loss_log_path = os.path.join(output_dir, \"compiled_epoch_losses.json\")\n",
    "    with open(loss_log_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"epoch_numbers\": epoch_numbers,\n",
    "            \"epoch_losses\": all_epoch_losses\n",
    "        }, f, indent=2)\n",
    "    print(f\"\\n📄 Saved compiled loss log to: {loss_log_path}\")\n",
    "\n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.lineplot(x=epoch_numbers, y=all_epoch_losses, marker=\"o\")\n",
    "    plt.title(\"📉 Training Loss Curve from Checkpoints\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"SmoothL1 Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"loss_curve_from_ckpts.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot deltas\n",
    "    if len(all_epoch_losses) > 1:\n",
    "        loss_deltas = [all_epoch_losses[i] - all_epoch_losses[i-1] for i in range(1, len(all_epoch_losses))]\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x=epoch_numbers[1:], y=loss_deltas)\n",
    "        plt.title(\"📊 Δ Loss Between Checkpoints\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Δ Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"loss_deltas_from_ckpts.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ Done analyzing {len(epoch_numbers)} checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_all_checkpoints(checkpoint_dir=model_checkpoint_dir, output_dir=analysis_output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039850f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(model_checkpoint_dir, k_neighbors=20, slice_embedding_dim=256):\n",
    "    \"\"\"\n",
    "    Loads the best model saved by the trainer.\n",
    "\n",
    "    Args:\n",
    "        model_checkpoint_dir (str): Path to the directory containing checkpoints.\n",
    "        k_neighbors (int): The 'k' value used for EdgeConv during training.\n",
    "        slice_embedding_dim (int): The embedding dimension of the slice encoder.\n",
    "    \"\"\"\n",
    "    # Find the best model file. Note: The name might vary based on ignite's saving.\n",
    "    # Let's search for any file starting with \"best_model\".\n",
    "    best_model_paths = glob.glob(os.path.join(model_checkpoint_dir, \"best_model_*.pt\"))\n",
    "    if not best_model_paths:\n",
    "        print(f\"❌ No best_model file found at: {model_checkpoint_dir}\")\n",
    "        return None\n",
    "    \n",
    "    best_model_path = best_model_paths[0] # Assume the first match is the one we want\n",
    "\n",
    "    # --- MODIFIED: Recreate the NEW model architecture ---\n",
    "    print(f\"[INFO] Recreating model with k={k_neighbors} to load best checkpoint...\")\n",
    "    slice_encoder = EdgeConvSliceEncoder(input_dim=2, emb_dim=slice_embedding_dim, k=k_neighbors)\n",
    "    # Ensure these Transformer/Regressor params match your training call\n",
    "    encoder = TransformerSliceEncoder(\n",
    "        input_dim=slice_embedding_dim,\n",
    "        hidden_dim=256,\n",
    "        num_layers=4,\n",
    "        nhead=4,\n",
    "    )\n",
    "    regressor = CdRegressor(input_dim=slice_embedding_dim)\n",
    "    best_model = CdPredictorNet(slice_encoder, encoder, regressor).to(device)\n",
    "\n",
    "    # Load the state dictionary\n",
    "    # The saved object from ModelCheckpoint is just the state_dict\n",
    "    best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    best_model.eval()\n",
    "    print(f\"🏆 Loaded best model from: {os.path.basename(best_model_path)}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_best_model(model_checkpoint_dir = model_checkpoint_dir):\n",
    "#     \"\"\"\n",
    "#     Loads the best model saved in `best_model.pt` using global architecture settings.\n",
    "#     \"\"\"\n",
    "#     best_model_path = os.path.join(model_checkpoint_dir, \"best_model.pt\")\n",
    "\n",
    "#     # Recreate model architecture using global config\n",
    "#     pointnet = PointNet2D(input_dim = 2, emb_dim=256)\n",
    "#     encoder = TransformerSliceEncoder(\n",
    "#         input_dim=256,\n",
    "#         hidden_dim=256,\n",
    "#         num_layers=4,\n",
    "#         nhead=4,\n",
    "#     )\n",
    "#     regressor = CdRegressor(input_dim=256)\n",
    "#     best_model = CdPredictorNet(pointnet, encoder, regressor).to(device)\n",
    "\n",
    "#     if os.path.exists(best_model_path):\n",
    "#         best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "#         best_model.eval()\n",
    "#         print(f\"🏆 Loaded best model from: {best_model_path}\")\n",
    "#     else:\n",
    "#         print(f\"❌ No best_model.pt found at: {best_model_path}\")\n",
    "\n",
    "#     return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c81863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the updated function with the correct k value\n",
    "best_model = load_best_model(checkpoints_dir, k_neighbors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd418c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# # Assumes `best_model` is already loaded and set to eval mode\n",
    "# assert best_model is not None, \"Best model is not loaded.\"\n",
    "\n",
    "# # Load test set IDs\n",
    "# id_file = \"../data/subset_dir/test_design_ids.txt\"\n",
    "# with open(id_file) as f:\n",
    "#     test_ids = [line.strip() for line in f]\n",
    "\n",
    "# # Ground-truth Cd map\n",
    "# df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "# cd_map = dict(zip(df[\"Design\"], df[\"Average Cd\"]))\n",
    "\n",
    "# # Evaluation setup\n",
    "# best_model.eval()\n",
    "# preds, trues, ids_used = [], [], []\n",
    "\n",
    "# for car_id in test_ids:\n",
    "#     npz_path = f\"../outputs/pad_masked_slices/{car_id}_axis-x.npz\"\n",
    "#     if not os.path.exists(npz_path):\n",
    "#         print(f\"⚠️ Missing file: {car_id}\")\n",
    "#         continue\n",
    "\n",
    "#     data = np.load(npz_path)\n",
    "#     slices = torch.tensor(data[\"slices\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "#     point_mask = torch.tensor(data[\"point_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "#     slice_mask = torch.tensor(data[\"slice_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         cd_scaled = best_model(slices, point_mask, slice_mask).item()\n",
    "#         cd_pred = scaler.inverse_transform([[cd_scaled]])[0, 0]\n",
    "\n",
    "#     cd_true = cd_map[car_id]\n",
    "#     preds.append(cd_pred)\n",
    "#     trues.append(cd_true)\n",
    "#     ids_used.append(car_id)\n",
    "\n",
    "#     print(f\"🚗 {car_id} → Predicted Cd: {cd_pred:.6f} | True Cd: {cd_true:.6f}\")\n",
    "\n",
    "# # Final metrics\n",
    "# r2 = r2_score(trues, preds)\n",
    "# mse = mean_squared_error(trues, preds)\n",
    "# mae = mean_absolute_error(trues, preds)\n",
    "# rmse = np.sqrt(mse)\n",
    "# mape = np.mean(np.abs((np.array(trues) - np.array(preds)) / np.array(trues))) * 100\n",
    "# nmae = np.mean(np.abs(np.array(trues) - np.array(preds)) / (np.max(trues) - np.min(trues))) * 100\n",
    "\n",
    "# print(\"\\n📊 Evaluation Summary:\")\n",
    "# print(f\"🔹 R² Score     : {r2:.4f}\")\n",
    "# print(f\"🔹 MSE          : {mse:.6f}\")\n",
    "# print(f\"🔹 RMSE         : {rmse:.6f}\")\n",
    "# print(f\"🔹 MAE          : {mae:.6f}\")\n",
    "# print(f\"🔹 MAPE         : {mape:.2f}%\")\n",
    "# print(f\"🔹 NMAE         : {nmae:.4f}\")\n",
    "# print(f\"🔹 Cars tested  : {len(preds)} / {len(test_ids)}\")\n",
    "\n",
    "# # Save predictions\n",
    "# os.makedirs(\"../outputs/eval2\", exist_ok=True)\n",
    "# out_path = f\"../outputs/eval2/cd_predictions_{os.path.basename(id_file).replace('.txt', '')}.csv\"\n",
    "# pd.DataFrame({\n",
    "#     \"Design ID\": ids_used,\n",
    "#     \"Predicted Cd\": preds,\n",
    "#     \"True Cd\": trues\n",
    "# }).to_csv(out_path, index=False)\n",
    "# print(f\"💾 Saved predictions to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fcf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "# Make sure a valid model was loaded\n",
    "if best_model is not None:\n",
    "    # Load test set IDs\n",
    "    id_file = \"../data/subset_dir/test_design_ids.txt\"\n",
    "    with open(id_file) as f:\n",
    "        test_ids = [line.strip() for line in f]\n",
    "\n",
    "    # Ground-truth Cd map\n",
    "    df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "    cd_map = dict(zip(df[\"Design\"], df[\"Average Cd\"]))\n",
    "\n",
    "    # --- MODIFIED: Point to raw data directory ---\n",
    "    raw_npy_dir = \"../outputs/slices\"\n",
    "\n",
    "    # Evaluation setup\n",
    "    best_model.eval()\n",
    "    preds, trues, ids_used = [], [], []\n",
    "\n",
    "    print(\"\\n[INFO] Starting evaluation on the test set...\")\n",
    "    for car_id in tqdm(test_ids, desc=\"Evaluating Test Set\"):\n",
    "        # --- MODIFIED: Load raw .npy file for the car ---\n",
    "        raw_npy_path = os.path.join(raw_npy_dir, f\"{car_id}_axis-x.npy\")\n",
    "        if not os.path.exists(raw_npy_path):\n",
    "            print(f\"⚠️ Missing file for car_id: {car_id}\")\n",
    "            continue\n",
    "\n",
    "        raw_slices = np.load(raw_npy_path, allow_pickle=True)\n",
    "\n",
    "        # --- MODIFIED: Prepare data in PyG format for a single car ---\n",
    "        # 1. Create a list of Data objects for the 80 slices\n",
    "        slice_data_list = [\n",
    "            Data(x=torch.from_numpy(s.astype(np.float32))) for s in raw_slices\n",
    "        ]\n",
    "        # 2. For each slice, create a \"batch of size 1\"\n",
    "        car_slice_batches = [Batch.from_data_list([data_obj]) for data_obj in slice_data_list]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # --- MODIFIED: Call the model with the new data format ---\n",
    "            cd_scaled = best_model(car_slice_batches).item()\n",
    "            cd_pred = scaler.inverse_transform([[cd_scaled]])[0, 0]\n",
    "\n",
    "        cd_true = cd_map.get(car_id)\n",
    "        if cd_true is not None:\n",
    "            preds.append(cd_pred)\n",
    "            trues.append(cd_true)\n",
    "            ids_used.append(car_id)\n",
    "        else:\n",
    "            print(f\"⚠️ Missing ground truth for car_id: {car_id}\")\n",
    "\n",
    "    # --- (The rest of the cell for metrics calculation remains IDENTICAL) ---\n",
    "    print(\"\\n📊 Evaluation Summary:\")\n",
    "    if trues:\n",
    "        r2 = r2_score(trues, preds)\n",
    "        mse = mean_squared_error(trues, preds)\n",
    "        mae = mean_absolute_error(trues, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((np.array(trues) - np.array(preds)) / np.array(trues))) * 100\n",
    "        nmae = np.mean(np.abs(np.array(trues) - np.array(preds)) / (np.max(trues) - np.min(trues))) * 100\n",
    "\n",
    "        print(f\"🔹 R² Score     : {r2:.4f}\")\n",
    "        print(f\"🔹 MSE          : {mse:.6f}\")\n",
    "        print(f\"🔹 RMSE         : {rmse:.6f}\")\n",
    "        print(f\"🔹 MAE          : {mae:.6f}\")\n",
    "        print(f\"🔹 MAPE         : {mape:.2f}%\")\n",
    "        print(f\"🔹 NMAE         : {nmae:.4f}\")\n",
    "        print(f\"🔹 Cars tested  : {len(preds)} / {len(test_ids)}\")\n",
    "\n",
    "        # Save predictions\n",
    "        os.makedirs(\"../outputs/eval_edgeconv\", exist_ok=True)\n",
    "        out_path = f\"../outputs/eval_edgeconv/cd_predictions_{os.path.basename(id_file).replace('.txt', '')}.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"Design ID\": ids_used,\n",
    "            \"Predicted Cd\": preds,\n",
    "            \"True Cd\": trues\n",
    "        }).to_csv(out_path, index=False)\n",
    "        print(f\"💾 Saved predictions to: {out_path}\")\n",
    "    else:\n",
    "        print(\"❌ No predictions were made. Check file paths and data.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping evaluation as the best model failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load full car IDs from training split\n",
    "with open(\"../data/subset_dir/train_design_ids.txt\") as f:\n",
    "    all_ids = [line.strip() for line in f]\n",
    "\n",
    "# Select 100 unseen IDs after first 1000 used in training\n",
    "test_ids = all_ids[5000:5100]\n",
    "\n",
    "# Load CSV for ground truth Cd values\n",
    "df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.eval()\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "\n",
    "for car_id in test_ids:\n",
    "    path = f\"../outputs/pad_masked_slices/{car_id}_axis-x.npz\"\n",
    "    data = np.load(path)\n",
    "\n",
    "    slices = torch.tensor(data[\"slices\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    point_mask = torch.tensor(data[\"point_mask\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    slice_mask = torch.tensor(data[\"slice_mask\"], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cd_pred = best_model(slices, point_mask, slice_mask).item()\n",
    "        cd_pred = scaler.inverse_transform([[cd_pred]])[0, 0]\n",
    "\n",
    "    cd_true = df[df[\"Design\"] == car_id][\"Average Cd\"].values[0]\n",
    "\n",
    "    preds.append(cd_pred)\n",
    "    trues.append(cd_true)\n",
    "\n",
    "    print(f\"🚗 {car_id} → Predicted Cd: {cd_pred:.4f} | True Cd: {cd_true:.4f}\")\n",
    "\n",
    "# Compute and print R²\n",
    "r2 = r2_score(trues, preds)\n",
    "print(f\"\\n📊 R² Score on unseen 100-car subset: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu-copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
